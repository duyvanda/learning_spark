1
00:00:01,150 --> 00:00:03,150
<v Instructor>So making movie recommendations</v>

2
00:00:03,150 --> 00:00:05,040
is a pretty tough thing to do with machine learning.

3
00:00:05,040 --> 00:00:07,810
So let's step back a little bit and go to a simpler example

4
00:00:07,810 --> 00:00:09,350
and that's linear regression.

5
00:00:09,350 --> 00:00:11,050
And we're gonna do that using spark.ml

6
00:00:11,050 --> 00:00:13,410
so we can scale this up to a massive data set

7
00:00:13,410 --> 00:00:14,243
if we wanted to.

8
00:00:14,243 --> 00:00:15,540
What is linear regression?

9
00:00:15,540 --> 00:00:16,640
It's really simple.

10
00:00:16,640 --> 00:00:18,060
All you're doing is fitting a line

11
00:00:18,060 --> 00:00:19,600
to a data set of observations

12
00:00:19,600 --> 00:00:22,250
and using that line to predict new values

13
00:00:22,250 --> 00:00:24,110
that you haven't seen before.

14
00:00:24,110 --> 00:00:25,580
So for example, in this graph,

15
00:00:25,580 --> 00:00:27,560
we are trying to predict people's height

16
00:00:27,560 --> 00:00:29,040
based on their weight.

17
00:00:29,040 --> 00:00:29,873
Generally speaking,

18
00:00:29,873 --> 00:00:32,450
the thing you're trying to predict is on the y-axis.

19
00:00:32,450 --> 00:00:33,700
So we have this set of data sets

20
00:00:33,700 --> 00:00:35,460
where we weight a bunch of people,

21
00:00:35,460 --> 00:00:36,470
and we measure their height,

22
00:00:36,470 --> 00:00:38,780
and then we made a plot of it on this scatterplot.

23
00:00:38,780 --> 00:00:40,120
And given this scatterplot,

24
00:00:40,120 --> 00:00:41,650
we can fit a line to it.

25
00:00:41,650 --> 00:00:43,150
And the idea here is to just

26
00:00:43,150 --> 00:00:45,950
try to find the line that minimizes the error

27
00:00:45,950 --> 00:00:48,630
between the distance between each one of those points

28
00:00:48,630 --> 00:00:50,380
and the distance to that line

29
00:00:50,380 --> 00:00:52,310
and that's how linear regression works.

30
00:00:52,310 --> 00:00:53,870
Once you have that line,

31
00:00:53,870 --> 00:00:55,960
you can use that to make new predictions.

32
00:00:55,960 --> 00:00:57,920
So you could take in someone who had a weight

33
00:00:57,920 --> 00:00:59,450
that we've never seen before

34
00:00:59,450 --> 00:01:01,660
and use that lined to predict their height.

35
00:01:01,660 --> 00:01:02,493
It's really simple.

36
00:01:02,493 --> 00:01:04,540
I mean, if you just remember high school level math,

37
00:01:04,540 --> 00:01:06,500
you can have an equation of a line

38
00:01:06,500 --> 00:01:10,130
in the form of a y-intercept and slope formula here

39
00:01:10,130 --> 00:01:12,777
and here, we're just saying we have a slope of 0.6

40
00:01:12,777 --> 00:01:14,770
and a y-intercept of 130.2

41
00:01:14,770 --> 00:01:16,690
so we just plug in the weight to that

42
00:01:16,690 --> 00:01:17,918
and you can predict a height.

43
00:01:17,918 --> 00:01:20,100
It's high school algebra stuff.

44
00:01:20,100 --> 00:01:21,300
I don't know why they call it regression.

45
00:01:21,300 --> 00:01:23,090
It's kind of a confusing name.

46
00:01:23,090 --> 00:01:25,770
Regression kind of implies that we're talking about

47
00:01:25,770 --> 00:01:27,700
looking at the past.

48
00:01:27,700 --> 00:01:29,280
In practice, for example,

49
00:01:29,280 --> 00:01:31,430
this example has nothing to do with time at all.

50
00:01:31,430 --> 00:01:34,640
So there are historical reasons why it's called regression,

51
00:01:34,640 --> 00:01:36,450
but don't let the name confuse you.

52
00:01:36,450 --> 00:01:39,530
Linear regression is just fitting a line to points

53
00:01:39,530 --> 00:01:41,480
and using that line to make predictions

54
00:01:41,480 --> 00:01:44,330
based on the previous points that you train that line to.

55
00:01:45,240 --> 00:01:46,073
How does it work?

56
00:01:46,073 --> 00:01:48,680
Well, uses a technique called least squares

57
00:01:48,680 --> 00:01:50,500
and even though that does sound familiar

58
00:01:50,500 --> 00:01:52,500
from what we just did with movie recommendations,

59
00:01:52,500 --> 00:01:54,640
it's a simpler algorithm.

60
00:01:54,640 --> 00:01:56,740
It's just minimizing the squared error

61
00:01:56,740 --> 00:01:59,090
between each point in the line like we talked about.

62
00:01:59,090 --> 00:02:00,060
Y squared-error,

63
00:02:00,060 --> 00:02:01,700
that's just so that positive and negative

64
00:02:01,700 --> 00:02:03,870
differences count the same really.

65
00:02:03,870 --> 00:02:07,230
And yeah, we can just take that resulting line,

66
00:02:07,230 --> 00:02:09,500
plug it into the slope intercept equation of the line

67
00:02:09,500 --> 00:02:11,360
y equals mx plus b

68
00:02:11,360 --> 00:02:14,140
and a little mathematical diversion here.

69
00:02:14,140 --> 00:02:16,040
It's kind of interesting that the slope

70
00:02:16,040 --> 00:02:19,030
ends up being the correlation between the two variables,

71
00:02:19,030 --> 00:02:20,680
times the standard deviation in y

72
00:02:20,680 --> 00:02:22,610
divided by the standard deviation in x.

73
00:02:22,610 --> 00:02:25,180
And I just find that cool because

74
00:02:25,180 --> 00:02:27,340
you kind of think of standard deviation sometimes

75
00:02:27,340 --> 00:02:29,340
as sort of this arbitrary statistical measure,

76
00:02:29,340 --> 00:02:32,670
but in fact, it has a real mathematical real world meaning

77
00:02:32,670 --> 00:02:35,210
and this really makes that evident.

78
00:02:35,210 --> 00:02:37,980
The y-intercept works out to be the mean of y

79
00:02:37,980 --> 00:02:39,910
minus the slope times the mean of x.

80
00:02:39,910 --> 00:02:44,760
So that's one way to compute the actual slope and intercept

81
00:02:44,760 --> 00:02:46,380
of this line that you're fitting.

82
00:02:46,380 --> 00:02:48,190
However, in the real world,

83
00:02:48,190 --> 00:02:49,510
things aren't that neat,

84
00:02:49,510 --> 00:02:51,460
you'll often have multiple dimensions,

85
00:02:51,460 --> 00:02:53,430
meaning that you'll have more than one feature

86
00:02:53,430 --> 00:02:55,200
that you're using to predict that label.

87
00:02:55,200 --> 00:02:57,580
So maybe you'd be predicting someone's height

88
00:02:57,580 --> 00:02:59,110
based not only on their weight,

89
00:02:59,110 --> 00:03:02,420
but their ethnicity or where they live

90
00:03:02,420 --> 00:03:04,330
or other things that might have

91
00:03:04,330 --> 00:03:05,920
a correlation to their height.

92
00:03:05,920 --> 00:03:08,300
When you're dealing with multi-dimensional spaces like that,

93
00:03:08,300 --> 00:03:10,050
you need fancier techniques.

94
00:03:10,050 --> 00:03:14,550
So the way that spark actually does linear regression,

95
00:03:14,550 --> 00:03:16,150
it says spark streaming on the slide here

96
00:03:16,150 --> 00:03:18,350
and there is a streaming version of this too.

97
00:03:18,350 --> 00:03:22,258
But it uses a technique called stochastic gradient descent,

98
00:03:22,258 --> 00:03:23,540
SGD for short.

99
00:03:23,540 --> 00:03:25,480
And this is a ubiquitous algorithm

100
00:03:25,480 --> 00:03:26,480
in the field of machine learning

101
00:03:26,480 --> 00:03:29,570
and also in deep learning and neural networks.

102
00:03:29,570 --> 00:03:30,990
This is not a course about that,

103
00:03:30,990 --> 00:03:32,640
so I'm not gonna get too much into depth about it,

104
00:03:32,640 --> 00:03:34,870
just know that it uses SGD under the hood

105
00:03:34,870 --> 00:03:36,823
to implement linear regression.

106
00:03:37,750 --> 00:03:39,660
Using it is simple,

107
00:03:39,660 --> 00:03:41,050
just like it was for ALS,

108
00:03:41,050 --> 00:03:43,640
the differences were gonna get better results this time.

109
00:03:43,640 --> 00:03:45,230
So you just set up your model like this,

110
00:03:45,230 --> 00:03:47,100
you can just say linear regression

111
00:03:47,100 --> 00:03:49,330
and pass into your hyper parameters,

112
00:03:49,330 --> 00:03:51,180
max iteration, regularization parameter,

113
00:03:51,180 --> 00:03:52,980
elastic net parameter.

114
00:03:52,980 --> 00:03:54,910
Again, a big part of the job is just gonna be

115
00:03:54,910 --> 00:03:56,740
finding the best hyper parameters

116
00:03:56,740 --> 00:03:58,200
to use for the best results

117
00:03:58,200 --> 00:04:00,650
and you can start from what you see in the examples online

118
00:04:00,650 --> 00:04:03,480
and that's usually a reasonable starting point.

119
00:04:03,480 --> 00:04:04,590
You'll train the model

120
00:04:04,590 --> 00:04:06,350
using your training data in a data frame

121
00:04:06,350 --> 00:04:09,380
and make predictions using tuples that consist of a label

122
00:04:09,380 --> 00:04:10,440
and a vector of features.

123
00:04:10,440 --> 00:04:11,273
So in this case,

124
00:04:11,273 --> 00:04:13,870
the label is the value we're trying to predict,

125
00:04:13,870 --> 00:04:15,740
that's usually on your y-axis

126
00:04:15,740 --> 00:04:17,740
and your feature will be on your x-axis

127
00:04:17,740 --> 00:04:20,330
or other axes if you want to visualize it.

128
00:04:20,330 --> 00:04:22,220
So you just train it with a bunch of known points,

129
00:04:22,220 --> 00:04:25,080
and you predict new label values for given features

130
00:04:25,080 --> 00:04:26,870
using the line that the model created.

131
00:04:26,870 --> 00:04:29,670
And again, this can work with more than two dimensions,

132
00:04:29,670 --> 00:04:33,290
that's the power of spark.ml's linear regression technique.

133
00:04:33,290 --> 00:04:35,870
There are some things to be aware of when using it though.

134
00:04:35,870 --> 00:04:37,720
First of all, stochastic gradient descent

135
00:04:37,720 --> 00:04:40,340
doesn't really handle feature scaling very well.

136
00:04:40,340 --> 00:04:42,470
So if you do have multiple features

137
00:04:42,470 --> 00:04:44,900
that are in different scales,

138
00:04:44,900 --> 00:04:45,800
that could be an issue.

139
00:04:45,800 --> 00:04:47,170
It's going to assume that your data

140
00:04:47,170 --> 00:04:49,130
is similar to a normal distribution.

141
00:04:49,130 --> 00:04:50,220
So generally speaking,

142
00:04:50,220 --> 00:04:52,430
you're gonna want to scale your data down

143
00:04:52,430 --> 00:04:53,760
to fit something that's closer

144
00:04:53,760 --> 00:04:56,480
to a normal distribution centered around zero,

145
00:04:56,480 --> 00:04:58,470
and then scale it back up again when you're done

146
00:04:58,470 --> 00:05:00,020
to get the best results.

147
00:05:00,020 --> 00:05:02,640
If you have different features that have different scales,

148
00:05:02,640 --> 00:05:06,100
like for example, if you're using people's weights,

149
00:05:06,100 --> 00:05:09,670
that might be a very different scale from how old they are.

150
00:05:09,670 --> 00:05:12,100
So those different scales can mess things up.

151
00:05:12,100 --> 00:05:14,140
You need to have this sort of uniform scale in place

152
00:05:14,140 --> 00:05:15,450
when you're training the model

153
00:05:15,450 --> 00:05:17,670
and that's a little bit of extra work.

154
00:05:17,670 --> 00:05:19,900
Also, it assumes that your y-intercept is zero

155
00:05:19,900 --> 00:05:20,750
on the resulting line,

156
00:05:20,750 --> 00:05:23,320
unless you call fit intercept true on it.

157
00:05:23,320 --> 00:05:25,020
So some things to be aware of.

158
00:05:25,020 --> 00:05:26,220
Let's go try it out.

159
00:05:26,220 --> 00:05:28,000
So as an experiment,

160
00:05:28,000 --> 00:05:31,080
we're going to fabricate some data for average page speed

161
00:05:31,080 --> 00:05:34,940
and revenue generated from session data on an online store.

162
00:05:34,940 --> 00:05:37,150
Now, the premise here is that we're trying to see

163
00:05:37,150 --> 00:05:38,910
if there's a relationship between

164
00:05:38,910 --> 00:05:40,730
how much money people spend

165
00:05:40,730 --> 00:05:43,370
and how fast their page load experience was

166
00:05:43,370 --> 00:05:45,530
while they were interacting with the website.

167
00:05:45,530 --> 00:05:46,760
And this is something we really did

168
00:05:46,760 --> 00:05:48,030
at Amazon a long time ago.

169
00:05:48,030 --> 00:05:49,170
This was before

170
00:05:49,170 --> 00:05:51,870
page speed was really a hot topic with Google and people.

171
00:05:51,870 --> 00:05:54,740
And the reason that it's such a hot topic even now

172
00:05:54,740 --> 00:05:56,750
is because yes, there is a correlation

173
00:05:56,750 --> 00:05:58,830
between how much people engage with your website

174
00:05:58,830 --> 00:06:00,760
and how much money they spend on your website

175
00:06:00,760 --> 00:06:03,410
and how responsive your website is.

176
00:06:03,410 --> 00:06:05,850
So can we predict revenue based on page speed

177
00:06:05,850 --> 00:06:07,540
using a linear regression model?

178
00:06:07,540 --> 00:06:09,913
Well, let's dive into the code and find out.

179
00:06:11,710 --> 00:06:13,220
So the code here is pretty simple.

180
00:06:13,220 --> 00:06:15,290
Most of the work is just in preparing our data

181
00:06:15,290 --> 00:06:17,220
for use with the model, actually.

182
00:06:17,220 --> 00:06:20,040
So we start off by creating our spark session.

183
00:06:20,040 --> 00:06:21,900
And by the way, this config line here

184
00:06:21,900 --> 00:06:23,680
for a spark.sql.warehouse.dir,

185
00:06:23,680 --> 00:06:25,850
that's a window specific thing.

186
00:06:25,850 --> 00:06:27,130
And it works around a bug

187
00:06:27,130 --> 00:06:29,060
that I believe they fixed in spark three,

188
00:06:29,060 --> 00:06:30,130
so you could actually get away

189
00:06:30,130 --> 00:06:33,180
with removing that particular clause there,

190
00:06:33,180 --> 00:06:35,530
if you want to just to simplify things.

191
00:06:35,530 --> 00:06:38,550
We'll call it LinearRegression and getOrCreate session.

192
00:06:38,550 --> 00:06:40,880
And we'll load up our regression.txt file

193
00:06:40,880 --> 00:06:42,740
that we're gonna use to train our model.

194
00:06:42,740 --> 00:06:45,180
So let's familiarize ourselves with that input data.

195
00:06:45,180 --> 00:06:47,520
That's always an important step in machine learning.

196
00:06:47,520 --> 00:06:49,133
Where is it regression.

197
00:06:50,260 --> 00:06:51,093
There it is.

198
00:06:52,210 --> 00:06:54,710
So you can see that we've already scaled this data down.

199
00:06:54,710 --> 00:06:56,400
So even though this is supposed to represent

200
00:06:56,400 --> 00:06:58,160
amount spent and page speed,

201
00:06:58,160 --> 00:07:02,352
we've normalized that down to a normal distributions scale,

202
00:07:02,352 --> 00:07:04,270
so we've already done the work of scaling everything down

203
00:07:04,270 --> 00:07:06,860
to the ranges that the linear regression algorithm

204
00:07:06,860 --> 00:07:08,620
works the best with.

205
00:07:08,620 --> 00:07:12,010
So we have two columns of data here separated by a comma.

206
00:07:12,010 --> 00:07:13,810
Let's see, what are we assuming that

207
00:07:14,680 --> 00:07:15,910
these correspond to.

208
00:07:15,910 --> 00:07:18,220
So let's take a look at the code here.

209
00:07:18,220 --> 00:07:19,890
We load up at the regression.txt file

210
00:07:19,890 --> 00:07:23,320
and then we map that splitting up based on the comma.

211
00:07:23,320 --> 00:07:25,500
Note that we're actually using the RDD interface here

212
00:07:25,500 --> 00:07:27,070
to parse that data out.

213
00:07:27,070 --> 00:07:29,790
In this case, it's a little bit more straightforward.

214
00:07:29,790 --> 00:07:31,970
So then we map x.

215
00:07:31,970 --> 00:07:34,660
X represents each row of that RDD.

216
00:07:34,660 --> 00:07:37,780
We extract the first column of information

217
00:07:37,780 --> 00:07:40,950
and then a dense vector of the remaining columns.

218
00:07:40,950 --> 00:07:42,560
So this is the format

219
00:07:42,560 --> 00:07:45,090
that the linear regression algorithm is expecting.

220
00:07:45,090 --> 00:07:47,270
It wants the first column to contain the label,

221
00:07:47,270 --> 00:07:48,260
the thing we're predicting,

222
00:07:48,260 --> 00:07:50,460
so in that case, it's gonna be amount spent.

223
00:07:50,460 --> 00:07:53,180
That's what that first column represents in this case.

224
00:07:53,180 --> 00:07:54,150
And then after that,

225
00:07:54,150 --> 00:07:56,690
we have a list of all the features that we're using.

226
00:07:56,690 --> 00:07:58,550
In our case, we only have one feature

227
00:07:58,550 --> 00:08:00,070
which is the page speed

228
00:08:00,070 --> 00:08:02,220
So going back to our data here,

229
00:08:02,220 --> 00:08:03,600
that first column is going to be

230
00:08:03,600 --> 00:08:06,280
a scaled down representation of amount spent,

231
00:08:06,280 --> 00:08:08,370
the thing we're trying to predict, the label,

232
00:08:08,370 --> 00:08:10,380
followed by all the features that we're using

233
00:08:10,380 --> 00:08:12,920
to make that prediction for the label.

234
00:08:12,920 --> 00:08:15,100
In this case, we only have one feature,

235
00:08:15,100 --> 00:08:17,500
which is a representation of the page speed,

236
00:08:17,500 --> 00:08:19,330
but you could have more than one.

237
00:08:19,330 --> 00:08:21,130
So you could create a dense vector

238
00:08:21,130 --> 00:08:22,490
of multiple features there

239
00:08:22,490 --> 00:08:26,470
and do a multiple dimension regression just as easily.

240
00:08:26,470 --> 00:08:27,340
Alright, moving on.

241
00:08:27,340 --> 00:08:29,070
So now that we have that RDD,

242
00:08:29,070 --> 00:08:31,080
we have to convert it to a DataFrame

243
00:08:31,080 --> 00:08:32,040
and we're gonna cheat there

244
00:08:32,040 --> 00:08:35,690
by just passing in this column names list here,

245
00:08:35,690 --> 00:08:37,690
and passing that into the toDF function

246
00:08:37,690 --> 00:08:42,080
to convert that RDD to a data frame with those column names.

247
00:08:42,080 --> 00:08:43,320
So pretty straightforward.

248
00:08:43,320 --> 00:08:45,710
So we're gonna end up with a label column

249
00:08:45,710 --> 00:08:48,290
that contains that floating point value

250
00:08:48,290 --> 00:08:50,310
that represents the amount spent,

251
00:08:50,310 --> 00:08:52,570
and then we're going to have a features column

252
00:08:52,570 --> 00:08:55,860
that contains a dense vector of feature information.

253
00:08:55,860 --> 00:08:58,040
In this case, only a single value

254
00:08:58,040 --> 00:09:00,090
will be inside that dense vector

255
00:09:00,090 --> 00:09:03,140
and that is our representation of page speed.

256
00:09:03,140 --> 00:09:05,050
All right, so that's the hard part.

257
00:09:05,050 --> 00:09:06,460
Now let's actually split this up

258
00:09:06,460 --> 00:09:09,050
into training data and testing data.

259
00:09:09,050 --> 00:09:11,040
So spark provides a handy way of doing that

260
00:09:11,040 --> 00:09:13,500
with the random split function on data frames

261
00:09:13,500 --> 00:09:16,880
and this will do a 50/50 split of that data

262
00:09:16,880 --> 00:09:20,350
and we'll use half of that data for training the model

263
00:09:20,350 --> 00:09:23,120
and half of that data for evaluating the model.

264
00:09:23,120 --> 00:09:25,660
The idea here is that you don't want to train the model

265
00:09:25,660 --> 00:09:27,700
on the same data that you're gonna test it with.

266
00:09:27,700 --> 00:09:29,530
It's kind of like cheating on a test.

267
00:09:29,530 --> 00:09:31,990
So that's what we're doing here.

268
00:09:31,990 --> 00:09:34,210
We extract train test zero and train test one

269
00:09:34,210 --> 00:09:38,270
and assign that to trainingDF and testDF respectively.

270
00:09:38,270 --> 00:09:40,890
Alright, now we're gonna create our linear regression model.

271
00:09:40,890 --> 00:09:42,280
Again, a lot of the work is gonna be

272
00:09:42,280 --> 00:09:44,010
in optimizing these hyper parameters.

273
00:09:44,010 --> 00:09:46,800
These are reasonable starting values however,

274
00:09:46,800 --> 00:09:47,633
but for best results,

275
00:09:47,633 --> 00:09:49,030
you're gonna want to try different values

276
00:09:49,030 --> 00:09:50,840
and see which ones work the best.

277
00:09:50,840 --> 00:09:53,180
We then train it using the training data frame.

278
00:09:53,180 --> 00:09:54,280
So that's the half of the data

279
00:09:54,280 --> 00:09:56,800
that we randomly assigned for training purposes.

280
00:09:56,800 --> 00:09:58,080
We just call fit on it

281
00:09:58,080 --> 00:10:00,980
and now we have a model that we can use to make predictions.

282
00:10:00,980 --> 00:10:02,090
So let's do that.

283
00:10:02,090 --> 00:10:05,850
We'll call model.transform passing in our test data.

284
00:10:05,850 --> 00:10:07,510
And we'll cash that set of results

285
00:10:07,510 --> 00:10:09,113
so that we can do stuff with it.

286
00:10:10,470 --> 00:10:13,000
So we need to print this out and see what we got.

287
00:10:13,000 --> 00:10:15,020
So we'll start by extracting those predictions

288
00:10:15,020 --> 00:10:16,740
and the known correct labels.

289
00:10:16,740 --> 00:10:18,980
And to get things back into a format

290
00:10:18,980 --> 00:10:20,340
that we can more easily process,

291
00:10:20,340 --> 00:10:23,110
we're going to convert those back to RDDs.

292
00:10:23,110 --> 00:10:26,090
So we'll start by selecting the prediction column,

293
00:10:26,090 --> 00:10:29,200
that's going to be the predicted amount spent

294
00:10:29,200 --> 00:10:30,490
and we we'll map that back

295
00:10:30,490 --> 00:10:32,470
to just have a single value in it.

296
00:10:32,470 --> 00:10:34,840
And similarly, we'll extract that single value

297
00:10:34,840 --> 00:10:36,820
out of the label column as well.

298
00:10:36,820 --> 00:10:38,530
All we're doing here is just simplifying this down

299
00:10:38,530 --> 00:10:41,090
to an easier structure to use within Python here.

300
00:10:41,090 --> 00:10:44,210
So we're taking what is sort of a complicated row object,

301
00:10:44,210 --> 00:10:46,920
extracting the one thing in that row that we care about

302
00:10:46,920 --> 00:10:48,660
and passing that back into our predictions

303
00:10:48,660 --> 00:10:50,640
and labels object here.

304
00:10:50,640 --> 00:10:52,660
We then call zip to zip them together.

305
00:10:52,660 --> 00:10:54,840
So this is just gonna collect together

306
00:10:54,840 --> 00:10:57,410
both the predicted and actual values for each point.

307
00:10:57,410 --> 00:10:59,530
This is a little bit challenging

308
00:10:59,530 --> 00:11:00,370
to wrap your head get around,

309
00:11:00,370 --> 00:11:01,760
but it's not important guys.

310
00:11:01,760 --> 00:11:02,870
This is not spark code.

311
00:11:02,870 --> 00:11:04,910
All we're doing is printing out the stuff

312
00:11:04,910 --> 00:11:06,900
in a way that makes sense for us to look at.

313
00:11:06,900 --> 00:11:08,440
So we're gonna iterate through each prediction

314
00:11:08,440 --> 00:11:11,240
in that prediction enabled zip together structure,

315
00:11:11,240 --> 00:11:13,540
print them out and then stop our session and we're done.

316
00:11:13,540 --> 00:11:15,340
So let's go ahead and kick this off.

317
00:11:21,010 --> 00:11:26,010
Our new script name here is spark linear-regression.PY.

318
00:11:27,630 --> 00:11:28,480
Let's kick it off

319
00:11:29,590 --> 00:11:31,123
and make some predictions.

320
00:11:32,870 --> 00:11:35,220
Now, the reason I kind of glossed over that whole zip thing

321
00:11:35,220 --> 00:11:37,100
and how we're tying together the final results

322
00:11:37,100 --> 00:11:38,030
is cause in the real world,

323
00:11:38,030 --> 00:11:40,010
you wouldn't really be doing it that way.

324
00:11:40,010 --> 00:11:42,940
You would actually be probably making some measure

325
00:11:42,940 --> 00:11:44,870
of the accuracy of those predictions instead,

326
00:11:44,870 --> 00:11:47,290
but I just want to simplify the results here

327
00:11:47,290 --> 00:11:49,970
so you can see them in a format that's easier to understand.

328
00:11:49,970 --> 00:11:52,440
So what we're seeing here is just the predicted value

329
00:11:52,440 --> 00:11:55,810
and the actual value for everything in our test data.

330
00:11:55,810 --> 00:11:57,790
So for every test point here,

331
00:11:57,790 --> 00:11:59,860
this is the value that we predicted

332
00:11:59,860 --> 00:12:01,590
the amount spent would be

333
00:12:01,590 --> 00:12:04,440
and the actual value that was in our test data.

334
00:12:04,440 --> 00:12:05,990
And just by eyeballing it,

335
00:12:05,990 --> 00:12:09,720
you can see, well, it's okay.

336
00:12:09,720 --> 00:12:11,030
It could be better.

337
00:12:11,030 --> 00:12:12,310
They're in the same ballpark at least.

338
00:12:12,310 --> 00:12:13,340
So that's cool.

339
00:12:13,340 --> 00:12:14,173
So it kind of worked.

340
00:12:14,173 --> 00:12:16,400
But the accuracy probably could be improved,

341
00:12:16,400 --> 00:12:18,380
either by throwing more data at it

342
00:12:18,380 --> 00:12:20,920
or maybe by playing with those hyper parameters.

343
00:12:20,920 --> 00:12:23,173
And this is to be fair, a fake data set.

344
00:12:24,140 --> 00:12:26,030
In the real world, you probably get much better results

345
00:12:26,030 --> 00:12:27,660
by using real world data,

346
00:12:27,660 --> 00:12:30,870
instead of data that are generated more or less at random.

347
00:12:30,870 --> 00:12:31,703
So there you have it,

348
00:12:31,703 --> 00:12:33,380
spark linear regression in action.

349
00:12:33,380 --> 00:12:35,780
As you can see, it's still really easy to use.

350
00:12:35,780 --> 00:12:37,550
The hard part is just in preparing your data

351
00:12:37,550 --> 00:12:39,230
for training it with and once you have that,

352
00:12:39,230 --> 00:12:42,760
it's very easy to generate a set of predictions

353
00:12:42,760 --> 00:12:45,583
using new data that you haven't seen before.

354
00:12:45,583 --> 00:12:48,980
So that is linear regression in spark.ml.

355
00:12:48,980 --> 00:12:50,710
As you can see, that works a little bit better

356
00:12:50,710 --> 00:12:53,020
and we get more reasonable results from that.

357
00:12:53,020 --> 00:12:54,470
Also, keep in mind too,

358
00:12:54,470 --> 00:12:56,930
that I skipped the step of scaling this backup.

359
00:12:56,930 --> 00:13:00,500
So remember that we scaled all of our training data down

360
00:13:00,500 --> 00:13:02,440
to be centered around zero

361
00:13:02,440 --> 00:13:06,770
and be more or less plus or minus oneish in range.

362
00:13:06,770 --> 00:13:09,250
We have to scale that back up to their proper range

363
00:13:09,250 --> 00:13:11,470
to get final results from here as well.

364
00:13:11,470 --> 00:13:14,320
These are all nuances of dealing with machine learning.

365
00:13:14,320 --> 00:13:15,530
Not really spark issues,

366
00:13:15,530 --> 00:13:18,230
but the important thing from a spark standpoint is

367
00:13:18,230 --> 00:13:20,470
building a linear regression model in spark

368
00:13:20,470 --> 00:13:22,660
is straightforward and using it is straightforward.

369
00:13:22,660 --> 00:13:24,060
That's what you got to know.

