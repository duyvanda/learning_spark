1
00:00:01,170 --> 00:00:05,880
So let's go ahead and walk through an example of using Spark streaming within Python here, and within

2
00:00:05,880 --> 00:00:09,480
your course materials you should have a couple of things associated with this example,

3
00:00:09,480 --> 00:00:14,010
one is the structured-streaming.py script that we're looking at right now and also the

4
00:00:14,010 --> 00:00:16,260
access_log.txt file.

5
00:00:16,260 --> 00:00:20,430
This is an actual Apache access log from one of the Web sites that I run.

6
00:00:20,430 --> 00:00:25,530
So what we're going to do is write a little script here that monitors a directory that has access logs

7
00:00:25,530 --> 00:00:26,480
being dumped into it.

8
00:00:26,510 --> 00:00:29,830
This is something you might actually see in the real world on a real web server right.

9
00:00:29,910 --> 00:00:35,940
And just keep monitoring that for new access logs and we want to keep a running count of how many status

10
00:00:35,940 --> 00:00:39,320
codes appear over time as this new log data is being adjusted.

11
00:00:39,480 --> 00:00:44,310
And this will let us do things like alert if we're seeing too many error codes are being generated by

12
00:00:44,310 --> 00:00:45,440
the server for example.

13
00:00:46,140 --> 00:00:50,040
So let's take a look through the actual script here and walk through it real quickly.

14
00:00:50,100 --> 00:00:55,740
So we start off by importing the various packages we need including SparkContext and also from pyspark.streaming,

15
00:00:55,740 --> 00:00:58,380
streaming context as well.

16
00:00:58,500 --> 00:01:02,970
And we're also going to import some functions for doing regular expressions so we can actually parse

17
00:01:02,970 --> 00:01:06,450
our Apache access logs into something more structured.

18
00:01:06,510 --> 00:01:08,510
We start by creating our Spark session.

19
00:01:08,610 --> 00:01:13,530
Nothing too special here we just call SparkSession.builder to do that with the following configuration

20
00:01:13,530 --> 00:01:14,390
options.

21
00:01:14,430 --> 00:01:17,720
This config is only necessary on windows by the way.

22
00:01:17,850 --> 00:01:21,400
Make sure you have a C temp directory created already if you don't already.

23
00:01:21,540 --> 00:01:23,370
And that's just to work around an issue on Windows.

24
00:01:23,400 --> 00:01:28,160
You'll find often that Windows support is kind of an afterthought with Spark.

25
00:01:28,170 --> 00:01:32,730
Sometimes things don't quite work properly on windows and this is a workaround for one of those things.

26
00:01:33,270 --> 00:01:38,100
We're gonna call the name StructuredStreaming and we'll call it getOrCreate to either create a new

27
00:01:38,100 --> 00:01:40,270
session or reuse an existing one.

28
00:01:40,380 --> 00:01:44,280
And this is important because we as we mentioned Spark can do check points.

29
00:01:44,310 --> 00:01:50,340
So if you did actually terminate this thing unexpectedly get or create could actually recreate the previous

30
00:01:50,340 --> 00:01:54,180
Spark session based on that checkpoint of data that had in place before.

31
00:01:54,180 --> 00:01:58,530
So if it did stop unexpectedly you could resume it by just running it again.

32
00:01:58,600 --> 00:02:00,750
Very nice feature, Sparks Streaming.

33
00:02:00,750 --> 00:02:08,280
So what we're gonna do is call spark.readStream.text("logs") what this does is create a new stream

34
00:02:08,580 --> 00:02:12,180
of data coming from the logs directory here on my file system.

35
00:02:12,180 --> 00:02:16,770
So it's just gonna sit there looking at our logs directory waiting for new text files to be dumped into

36
00:02:16,770 --> 00:02:21,390
it and it will stream that new data as it's found into access lines.

37
00:02:21,390 --> 00:02:26,100
Now one thing that's worth noting is that during development I could just say spark.read instead

38
00:02:26,100 --> 00:02:31,200
of readStream and write all the same code just using a static data frame instead of a streaming one.

39
00:02:31,200 --> 00:02:37,140
So again it's very useful for development purposes where you can just switch from a static data frame

40
00:02:37,140 --> 00:02:40,770
to a streaming one by just changing read to readStream when you're done.

41
00:02:40,770 --> 00:02:44,430
So that does make life a little bit simpler for testing and development.

42
00:02:44,430 --> 00:02:49,110
Once we have that we need to convert this into a data frame that has the structure that we want.

43
00:02:49,110 --> 00:02:50,880
So this is problematic.

44
00:02:50,880 --> 00:02:56,130
Like right now we just have individual lines from our access log and they're kind of messy to look at

45
00:02:56,140 --> 00:02:57,630
if we look at an example.

46
00:02:57,690 --> 00:03:01,890
Let's go back to our materials here and open up the access logs text file just to see what we're up

47
00:03:01,890 --> 00:03:04,620
against.

48
00:03:04,640 --> 00:03:06,680
So this is what an Apache axis log looks like.

49
00:03:06,680 --> 00:03:11,050
Basically it starts with an IP address of the requester it has a timestamp,

50
00:03:11,180 --> 00:03:16,640
it has the actual request that was made of HTTP which is usually getSomeURL over this

51
00:03:16,640 --> 00:03:17,460
protocol.

52
00:03:17,660 --> 00:03:18,830
And then we have the status code.

53
00:03:18,830 --> 00:03:23,480
So two hundred indicates success and a bunch of other stuff like "refers" and things like that.

54
00:03:23,480 --> 00:03:27,180
So we need to parse these lines out into their structure.

55
00:03:27,230 --> 00:03:29,300
So that's what the following code does.

56
00:03:29,570 --> 00:03:32,960
Close out of this. I don't want to get into the details of regular expressions.

57
00:03:32,960 --> 00:03:34,750
I mean that will be a whole course of itself.

58
00:03:34,760 --> 00:03:40,580
But just take it on faith that what this code does is parse out each individual log line and break it

59
00:03:40,580 --> 00:03:42,680
up into its individual components.

60
00:03:42,680 --> 00:03:47,960
So by saying accessLines.select with all these regular expressions, we're saying use these patterns

61
00:03:47,960 --> 00:03:54,200
to match patterns in that access log line to extract the hosts, the timestamp, the method, the endpoint, the

62
00:03:54,200 --> 00:03:59,980
protocol, the status code, and the content size based on these patterns that we've defined here.

63
00:04:00,190 --> 00:04:05,540
And as we do that we're going to generate a new data frame called logsDF that contains these host

64
00:04:05,540 --> 00:04:09,020
timestamp method et cetera aliases as column names.

65
00:04:09,470 --> 00:04:13,400
So at that point we can just refer to this data frame as we would any data frame,

66
00:04:13,400 --> 00:04:18,500
the only difference is that because it's streaming we're appending new information to it over time continuously.

67
00:04:18,500 --> 00:04:24,140
So, pretty cool how that works. Now that we have the data frame handy we can just say group that data

68
00:04:24,140 --> 00:04:27,290
frame by the status code and keep it running CAF read status code.

69
00:04:27,440 --> 00:04:33,050
So with one line of code we said keep track of everything by status code with the "group by" command and

70
00:04:33,050 --> 00:04:34,410
keep them up over time.

71
00:04:34,430 --> 00:04:37,380
So it's a very SQL style format there,

72
00:04:37,400 --> 00:04:40,790
in fact you could just make that a SQL command if you preferred.

73
00:04:40,940 --> 00:04:44,660
Once we have that defined we basically told it what we wanted to do, we want it to count up status

74
00:04:44,660 --> 00:04:45,730
codes over time.

75
00:04:45,920 --> 00:04:47,970
We just kick it off with this line here.

76
00:04:48,140 --> 00:04:52,940
We say take that status counts data frame that we've defined, our final results there, and call write

77
00:04:52,940 --> 00:04:54,920
stream to dump the results somewhere.

78
00:04:54,920 --> 00:05:00,860
So in this case we're just gonna dump the complete output to the console and we're gonna have that query

79
00:05:00,860 --> 00:05:04,490
a name called "counts" and calls "start" to actually kick it off.

80
00:05:04,490 --> 00:05:09,200
So that will actually start our structured streaming application, where we have this status counts data

81
00:05:09,200 --> 00:05:14,090
frame that just keeps expanding and changing over time based on what status codes that it sees coming

82
00:05:14,090 --> 00:05:18,970
in, and we'll just run that forever until termination is detected,

83
00:05:18,970 --> 00:05:23,870
and at that point we have to cleanly shut down our Sparks session to let things know that it shut down

84
00:05:23,870 --> 00:05:24,500
successfully.

85
00:05:24,500 --> 00:05:27,700
We don't need to like recover that from a snapshot later on.

86
00:05:27,740 --> 00:05:29,280
So let's see if it works.

87
00:05:29,360 --> 00:05:34,050
So we'll go back to our Canopy command prompt.

88
00:05:34,070 --> 00:05:39,440
Actually already have one open here so I'm just gonna go back to it and we will type in

89
00:05:42,410 --> 00:05:43,780
spark-submit structured-streaming.py

90
00:05:43,790 --> 00:05:47,690
but before you hit enter make sure you have a log structure for it to monitor or else the code won't

91
00:05:47,690 --> 00:05:48,530
work.

92
00:05:48,530 --> 00:05:50,950
So back to our course materials here.

93
00:05:51,220 --> 00:05:54,700
I already have a logs directory here that's created, there should be nothing in it at this point

94
00:05:54,860 --> 00:05:59,340
but if you need to create a logs directory first please take care of that right now.

95
00:05:59,420 --> 00:06:01,790
Once we have a log structure in place we can actually kick this off.

96
00:06:01,790 --> 00:06:03,260
So let's do so.

97
00:06:03,260 --> 00:06:09,660
Give that chance to spin up and just get Spark started and let's get some data to actually play with.

98
00:06:09,690 --> 00:06:12,390
So let's put this off to the side here.

99
00:06:12,720 --> 00:06:19,980
And what I'm going to do is take that access log.txt file, hit CTRL+C to copy it, and let's

100
00:06:19,980 --> 00:06:22,370
go to our logs directory and paste it in.

101
00:06:22,680 --> 00:06:26,640
And that should trigger our application to do something, it's going to say "oh this logs directory that

102
00:06:26,640 --> 00:06:28,410
I'm monitoring has some new information in it,

103
00:06:28,530 --> 00:06:29,440
let's process it."

104
00:06:29,490 --> 00:06:31,500
So give it a few seconds to actually pick that up.

105
00:06:38,170 --> 00:06:43,590
And there we have our first batch of information there, so we can see that we have quite a few five hundreds

106
00:06:43,590 --> 00:06:43,780
there.

107
00:06:43,780 --> 00:06:48,130
So turns out something is actually going wrong there but that's successfully counting up the number of

108
00:06:48,130 --> 00:06:52,240
status codes within this entire access log file here and it's quite a bit of information actually.

109
00:06:52,240 --> 00:06:55,820
We've got about 10 k of information there.

110
00:06:55,840 --> 00:07:00,110
Let's go ahead and see if we get further data here just to demonstrate that it is in fact streaming.

111
00:07:00,130 --> 00:07:06,400
So let's go back to our course materials here and let's rename access_log.txt to something else,

112
00:07:06,760 --> 00:07:14,250
underscore one perhaps, and copy that new file, which is really a copy of the original one, into our logs

113
00:07:14,250 --> 00:07:19,100
directory, and what I would expect to see is all of these counts to double right.

114
00:07:19,130 --> 00:07:24,020
Because now we've added the same information in twice but we're still monitoring the stream of information.

115
00:07:24,020 --> 00:07:28,190
So it should say "hey I just saw some new information come into my logs directory,

116
00:07:28,310 --> 00:07:32,710
I better go process it". And there it is.

117
00:07:32,770 --> 00:07:35,230
And sure enough everything is double what it was before.

118
00:07:35,230 --> 00:07:37,780
So hey structured streaming actually works.

119
00:07:37,780 --> 00:07:42,670
We just have this giant data frame keeping track of all the information coming in and distilling it

120
00:07:42,670 --> 00:07:47,860
down to this account data frame that's keeping track of all the different status codes and how often

121
00:07:47,860 --> 00:07:48,820
they've appeared over time.

122
00:07:48,880 --> 00:07:53,920
So again very easy very clean API and you can kind of see why they're pushing people in this direction

123
00:07:53,920 --> 00:07:55,350
as opposed to Dstreams.

124
00:07:55,390 --> 00:07:59,980
Not only is it a cleaner API for working with, it also makes development easier,

125
00:07:59,980 --> 00:08:06,130
and it also increases in operability with other parts of Spark and beyond that it's real time streaming

126
00:08:06,130 --> 00:08:08,740
or about as near real time as you can get anyway.

127
00:08:08,890 --> 00:08:11,080
It's not dealing with discrete batches anymore.

128
00:08:11,230 --> 00:08:12,120
It's really streaming.

129
00:08:12,130 --> 00:08:15,400
So cool stuff, that is Spark Streaming in action within Python.

130
00:08:15,400 --> 00:08:20,440
There's a whole lot more to Spark Streaming to explore, in fact I have a whole other course on it, but

131
00:08:20,440 --> 00:08:25,120
that should give you a feel of how it all works and a nice little working example here with Sparks Streaming

132
00:08:25,750 --> 00:08:26,970
within Python.

133
00:08:27,010 --> 00:08:31,540
It turns out the examples that are included in the Python SDK currently don't actually include a Spark

134
00:08:31,540 --> 00:08:34,050
Streaming example so, you might want to keep this one around.

135
00:08:34,060 --> 00:08:39,910
It could be a handy reference for you. When you're done just hit CTRL+C here and terminate that.

136
00:08:41,070 --> 00:08:46,490
You'll have to terminate it possibly two or more times for reach core that's running and then you're done.
