1
00:00:00,910 --> 00:00:06,310
So we've talked about Spark Core and AML lib and also Sparks SQL which are to the boxes built on top

2
00:00:06,310 --> 00:00:06,930
of Spark Core.

3
00:00:06,940 --> 00:00:11,450
Let's talk about those other two boxes Spark Streaming and GraphX.

4
00:00:11,470 --> 00:00:14,890
We'll start with Spark Streaming that's definitely the bigger of the two and the one that's more widely

5
00:00:14,890 --> 00:00:19,870
used Spark Streaming is used for analyzing continual streams of data.

6
00:00:19,990 --> 00:00:23,890
So sometimes you don't just have this big corpus of data just sitting there that you want to analyze

7
00:00:23,890 --> 00:00:28,490
as a big batch, you have a continual stream of data that's always coming in all the time.

8
00:00:28,660 --> 00:00:33,490
And a common example of that is the problem of processing log data that's being generated by a Web site

9
00:00:33,490 --> 00:00:38,560
or a server somewhere, where that server just keeps on generating log data forever and you need to monitor

10
00:00:38,560 --> 00:00:43,870
that thing in real time or near real time at least. For example maybe you need to keep a watch on error

11
00:00:43,870 --> 00:00:48,490
codes that are being generated and you want to generate some sort of an alarm or pay somebody or alert

12
00:00:48,490 --> 00:00:52,020
somebody when things go wrong, Sparks Streaming can do that.

13
00:00:52,030 --> 00:00:57,250
The idea is to aggregate and analyze data as it's being streamed in at some given interval, and you can

14
00:00:57,250 --> 00:00:59,850
take that data from pretty much anywhere,

15
00:00:59,860 --> 00:01:03,810
I mean at a low level you can listen to data that's just fed in on some arbitrary port.

16
00:01:03,820 --> 00:01:09,430
So if it can talk to TCP you can feed it into Sparks Streaming. But you can also deal with integrations

17
00:01:09,430 --> 00:01:12,130
with specific systems to make things a little bit easier.

18
00:01:12,130 --> 00:01:17,830
For example Amazon Kinesis or listening to data that's being dropped in an HDFS file system or data

19
00:01:17,830 --> 00:01:21,160
that's being generated by Kafka or Flume and other systems.

20
00:01:21,160 --> 00:01:25,990
These things all have APIs and libraries that you can use to talk to them directly and consume that

21
00:01:25,990 --> 00:01:31,360
data into Sparks Streaming, crunch that data on your Spark cluster and then take the results and feed

22
00:01:31,360 --> 00:01:36,910
it off to another stream or act on it in some way stored in a database whatever you want to do. It also

23
00:01:36,910 --> 00:01:39,310
has some nice features for a fault tolerance,

24
00:01:39,310 --> 00:01:44,440
for example it has a check pointing feature that will continually store its current state to disk periodically.

25
00:01:44,890 --> 00:01:49,570
So that way if your stream goes down or something goes wrong with the entire system, it can pick up where

26
00:01:49,570 --> 00:01:52,810
it left off automatically the next time you get it running again,

27
00:01:52,840 --> 00:01:55,810
so that's a good feature of Spark Streaming.

28
00:01:55,810 --> 00:02:00,370
These are complicated systems that involve you know multiple systems that involve clusters of their

29
00:02:00,370 --> 00:02:00,600
own.

30
00:02:00,610 --> 00:02:05,480
So there's a lot that can go wrong so that's an important feature to have. There's a couple of ways of

31
00:02:05,480 --> 00:02:06,970
doing Spark Streaming.

32
00:02:06,980 --> 00:02:12,500
The old way that sort of a historical way of doing it is using what we call a Dstream object.

33
00:02:12,500 --> 00:02:17,340
And the idea of a Dstream is that your stream is broken up into distinct RDD's.

34
00:02:17,390 --> 00:02:22,550
So technically it's not a real time stream of information with a Dstream, you're getting what we call

35
00:02:22,550 --> 00:02:28,250
micro batches, just little snapshots of each chunk of data that's coming in and you deal with each chunk

36
00:02:28,250 --> 00:02:30,690
at a time as a distinct RDD.

37
00:02:30,740 --> 00:02:32,570
A simple example of that is here on the slide,

38
00:02:32,630 --> 00:02:36,980
so this is a streaming word count example that comes straight out of the examples that ship with the

39
00:02:36,980 --> 00:02:38,660
Spark SDK.

40
00:02:38,660 --> 00:02:42,830
You start with the Spark context and from there you can create a streaming context built on top of that

41
00:02:42,830 --> 00:02:44,120
Spark context.

42
00:02:44,240 --> 00:02:48,650
So the parameters on the first line indicate that we're creating a streaming context from a Spark context

43
00:02:48,650 --> 00:02:52,770
named sc and the 1 means that we want to break that up into one second chunk.

44
00:02:52,790 --> 00:02:59,030
So we're gonna be taking one second periods of that data, making individual RDD's out of each one second

45
00:02:59,030 --> 00:03:02,690
worth of data and then processing those RDD's individually.

46
00:03:02,690 --> 00:03:05,890
So at this point we're assuming that we're dealing with an individual RDD,

47
00:03:05,930 --> 00:03:12,470
and we're gonna call an RDD call line, it's going to represent a 1 second snapshot of a text file stream

48
00:03:12,590 --> 00:03:14,150
coming from the books directory.

49
00:03:14,150 --> 00:03:19,090
So text file stream just listens to a directory somewhere in a file system, in this case maybe it's an

50
00:03:19,090 --> 00:03:24,260
HDFS file system where it has a directory called Books and we'll just sit there monitoring that books

51
00:03:24,260 --> 00:03:29,900
directory looking for new data being dumped into it over time, as new data comes in within each one second

52
00:03:29,900 --> 00:03:34,580
intervals will process the data that it sees within that one second and feed that back into your lines

53
00:03:34,580 --> 00:03:35,900
RDD here.

54
00:03:35,930 --> 00:03:37,040
From there we can process it.

55
00:03:37,130 --> 00:03:45,040
So this line here is just some Python Spark code to count up the number of occurrences of each word

56
00:03:45,050 --> 00:03:46,220
within that RDD.

57
00:03:46,250 --> 00:03:50,960
So we start off with a lambda function that just splits each line in to individual words based on the

58
00:03:50,960 --> 00:03:52,340
space character.

59
00:03:52,340 --> 00:03:54,720
We then map that to a count,

60
00:03:54,740 --> 00:03:59,990
so we have a mapping of each individual word to the number 1 and then we just add up all those ones

61
00:03:59,990 --> 00:04:03,230
together to get a final count of each word with reduced by key.

62
00:04:03,230 --> 00:04:08,050
So that's just keeping a count of all the individual words how often they appear within that RDD,

63
00:04:08,180 --> 00:04:10,400
we then print out the results and that's it.

64
00:04:10,400 --> 00:04:15,290
So those four lines of code are pretty much all you need to monitor a directory and keep a running count

65
00:04:15,290 --> 00:04:19,220
of how many words appear within a given one second increment of information.

66
00:04:20,000 --> 00:04:22,530
Once you have that you do need to kick off the job explicitly.

67
00:04:22,550 --> 00:04:26,870
You just need to call scc.start to kick off monitoring that books folder there and actually start

68
00:04:26,870 --> 00:04:32,030
the stream and then you call awaitTermination to just say go and run this repeatedly forever until

69
00:04:32,030 --> 00:04:33,050
I say stop.

70
00:04:33,110 --> 00:04:34,520
That's all there is to it.

71
00:04:34,520 --> 00:04:36,890
So Dstreams again are near real time,

72
00:04:36,900 --> 00:04:42,020
there they're a micro batch API where we're dealing with chunks of data being streamed and continuously

73
00:04:42,350 --> 00:04:45,740
but they're not it's not technically real time streaming.

74
00:04:45,820 --> 00:04:46,050
OK.

75
00:04:46,100 --> 00:04:50,360
So some people find that a little bit off putting but it's kind of hard to find an application where

76
00:04:50,360 --> 00:04:52,100
a 1 second delay is really a big deal.

77
00:04:52,100 --> 00:04:55,930
So in a lot of cases a Dstream is a perfectly legitimate way to go.

78
00:04:56,030 --> 00:04:57,080
Totally fine.

79
00:04:57,080 --> 00:05:03,130
And actually the examples that ship with Spark itself for Python streaming all use this Dstream API still.

80
00:05:03,130 --> 00:05:08,240
So you know even though there is a newer version- a newer API that we're going to look at shortly, they

81
00:05:08,240 --> 00:05:15,200
still seem to be focusing on this in the Python world. now remember your RDD's with the Dstream only contain

82
00:05:15,200 --> 00:05:17,370
that one little chunk of incoming data.

83
00:05:17,450 --> 00:05:21,710
Now there are ways to aggregate data across multiple RDD's, these multiple chunks and actually keep

84
00:05:21,710 --> 00:05:23,380
track of things over time.

85
00:05:23,420 --> 00:05:27,980
We have something called windowed operations where you can actually combine results from multiple batches

86
00:05:27,980 --> 00:05:33,470
over some sliding time window. So there are these functions like window and reduce by window and reduce

87
00:05:33,470 --> 00:05:38,690
by key n window that you could use to do that word count example over a sliding window say the past

88
00:05:38,690 --> 00:05:42,760
hours worth of data or the past day's worth of data whatever you want whenever window of time you want.

89
00:05:43,220 --> 00:05:45,830
So you can do that pretty easily with Dstreams.

90
00:05:45,980 --> 00:05:50,330
You can also maintain any arbitrary state you want across these different batches that come in from

91
00:05:50,330 --> 00:05:54,290
your streaming data, updateStateByKey is what lets you do that.

92
00:05:54,410 --> 00:05:59,000
That just lets you keep track of some sort of state, some arbitrary information across many batches as

93
00:05:59,000 --> 00:05:59,750
time goes on.

94
00:05:59,780 --> 00:06:04,240
So for example if you wanted to keep a running count of some advent or like the total number of lines

95
00:06:04,240 --> 00:06:09,220
that you've processed since the beginning of the stream, you could use updateStateByKey to do that.

96
00:06:09,730 --> 00:06:15,140
And if you want to see an example of that in action just navigate into the Python examples that ship

97
00:06:15,140 --> 00:06:20,360
with the spark SDK, you'll see a file called stateful_network_wordcount that will show you how that all

98
00:06:20,360 --> 00:06:20,780
works.

99
00:06:21,710 --> 00:06:24,900
But the new way of doing all this stuff is called structured streaming.

100
00:06:24,900 --> 00:06:29,510
And the idea there is that instead of using Dstreams and these distinct RDD's of data, we're just going

101
00:06:29,510 --> 00:06:33,920
to model our stream as a data frame that just keeps on growing over time infinitely.

102
00:06:33,920 --> 00:06:36,000
So this is kind of the new way of doing it.

103
00:06:36,080 --> 00:06:40,760
Python support for structured streaming is still relatively recent but this is the direction that they're

104
00:06:41,060 --> 00:06:46,250
pushing their Scala customers in and really people in general this is kind of the the new fangled way

105
00:06:46,250 --> 00:06:47,640
of doing streaming.

106
00:06:47,690 --> 00:06:52,760
The idea is that you just have a data stream feeding into this unbounded table, this unbounded data frame and

107
00:06:52,760 --> 00:06:57,980
as new data arrives it just results in new rows being appended to that input table. And there are a few

108
00:06:57,980 --> 00:06:59,980
advantages of this way of thinking,

109
00:06:59,980 --> 00:07:04,730
so one is because you can just think of it as a data frame and process it like any other data frame,

110
00:07:05,180 --> 00:07:08,990
your streaming code ends up looking a lot like the non streaming code for dealing with the same kind

111
00:07:08,990 --> 00:07:09,830
of data.

112
00:07:09,980 --> 00:07:11,640
So it makes development a little bit easier.

113
00:07:11,660 --> 00:07:15,730
You can start with just a static data frame of data that's sitting on your harddrive somewhere, write

114
00:07:15,740 --> 00:07:20,210
all the code for processing and analyzing and doing whatever you want to that data frame and then it's

115
00:07:20,210 --> 00:07:23,670
a very slight modification to modify that to deal the stream of data instead.

116
00:07:23,710 --> 00:07:26,050
So it makes development a little bit easier.

117
00:07:26,090 --> 00:07:31,130
It's also useful because other systems within Spark are also moving toward data frames as an

118
00:07:31,130 --> 00:07:37,520
API, so MLLib again is also moving toward data frames as its primary interface and especially in Spark

119
00:07:37,540 --> 00:07:37,790
3,

120
00:07:37,790 --> 00:07:42,050
that's definitely the direction they're going. In fact the RDD interfaces will probably go away sometime

121
00:07:42,050 --> 00:07:43,430
soon entirely.

122
00:07:43,430 --> 00:07:48,110
And the nice thing is that by using data frames as the API to structured streaming, we can interoperate

123
00:07:48,110 --> 00:07:53,210
more easily with things like MLLib, so it would be simple for example to take a streaming data source

124
00:07:53,210 --> 00:07:58,470
and feed that into a machine learning library algorithm using a data frame API.

125
00:07:58,470 --> 00:08:02,450
With that let's look at an example and make it real and show you that this stuff actually works.
